<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Linear Regression</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Statistics Notebook</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Describing Data
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="GraphicalSummaries.html">Graphical Summaries</a>
    </li>
    <li>
      <a href="NumericalSummaries.html">Numerical Summaries</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    R Help
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="RCommands.html">R Commands</a>
    </li>
    <li>
      <a href="RMarkdownHints.html">R Markdown Hints</a>
    </li>
    <li>
      <a href="RCheatSheetsAndNotes.html">R Cheatsheets &amp; Notes</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Making Inference
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="MakingInference.html">Making Inference</a>
    </li>
    <li>
      <a href="tTests.html">t Tests</a>
    </li>
    <li>
      <a href="WilcoxonTests.html">Wilcoxon Tests</a>
    </li>
    <li>
      <a href="ANOVA.html">ANOVA</a>
    </li>
    <li>
      <a href="Kruskal.html">Kruskal-Wallis</a>
    </li>
    <li>
      <a href="LinearRegression.html">Linear Regression</a>
    </li>
    <li>
      <a href="LogisticRegression.html">Logistic Regression</a>
    </li>
    <li>
      <a href="ChiSquaredTests.html">Chi Squared Tests</a>
    </li>
    <li>
      <a href="PermutationTests.html">Randomization Testing</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Analyses
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="./Analyses/AnalysisRubric.html">Analysis Rubric</a>
    </li>
    <li>
      <a href="./Analyses/StudentHousing.html">Good Example Analysis</a>
    </li>
    <li>
      <a href="./Analyses/StudentHousingPOOR.html">Poor Example Analysis</a>
    </li>
    <li>
      <a href="./Analyses/Stephanie.html">Stephanie Analysis</a>
    </li>
    <li>
      <a href="./Analyses/Olympics.html">Olympics Analysis</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Linear Regression</h1>

</div>


<script type="text/javascript">
 function showhide(id) {
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
 }
 
 function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
 }
</script>
<hr />
<p>Determine which explanatory variables have a significant effect on the mean of the quantitative response variable.</p>
<hr />
<div id="simple-linear-regression" class="section level2 tabset tabset-fade tabset-pills">
<h2>Simple Linear Regression</h2>
<div style="float:left;width:125px;" align="center">
<p><img src="Images/QuantYQuantX.png" width=58px;></p>
</div>
<p>Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable <span class="math inline">\(Y\)</span> and a single quantitative explanatory variable <span class="math inline">\(X\)</span>.</p>
<div id="overview" class="section level3 tabset">
<h3>Overview</h3>
<div style="padding-left:125px;">
<p><strong>Mathematical Model</strong></p>
<p>The true regression model assumed by a regression analysis is given by <span class="math display">\[
  Y_i = \underbrace{\overbrace{\beta_0}^\text{y-intercept} + \overbrace{\beta_1}^\text{slope} X_i \ }_\text{true regression relation} + \overbrace{\epsilon_i}^\text{error term} \quad \text{where} \ \overbrace{\epsilon_i \sim N(0, \sigma^2)}^\text{error term normally distributed}
\]</span></p>
<p>The estimated regression line obtained from a regression analysis, pronounced “y-hat”, is written as</p>
<p><span class="math display">\[
  \hat{Y}_i = \underbrace{\overbrace{\ b_0 \ }^\text{y-intercept} + \overbrace{b_1}^\text{slope} X_i \ }_\text{estimated regression relation}
\]</span></p>
<div style="font-size:0.8em;">
<p>Note: see the <strong>Explanation</strong> tab for details about these equations.</p>
</div>
<p><strong>Hypotheses</strong></p>
<p><span class="math display">\[
\left.\begin{array}{ll}
H_0: \beta_1 = 0 \\  
H_a: \beta_1 \neq 0
\end{array}
\right\} \ \text{Slope Hypotheses}^{\quad \text{(most common)}}\quad\quad
\]</span></p>
<p><span class="math display">\[
\left.\begin{array}{ll}
H_0: \beta_0 = 0 \\  
H_a: \beta_0 \neq 0
\end{array}
\right\} \ \text{Intercept Hypotheses}^{\quad\text{(sometimes useful)}}
\]</span></p>
<p>If <span class="math inline">\(\beta_1 = 0\)</span>, then the model reduces to <span class="math inline">\(Y_i = \beta_0 + \epsilon_i\)</span>, which is a flat line. This means <span class="math inline">\(X\)</span> does not improve our understanding of the mean of <span class="math inline">\(Y\)</span> if the null hypothesis is true.</p>
<p>If <span class="math inline">\(\beta_0 = 0\)</span>, then the model reduces to <span class="math inline">\(Y_i = \beta_1 X + \epsilon_i\)</span>, a line going through the origin. This means the average <span class="math inline">\(Y\)</span>-value is <span class="math inline">\(0\)</span> when <span class="math inline">\(X=0\)</span> if the null hypothesis is true.</p>
<p><strong>Assumptions</strong></p>
<p>This regression model is appropriate for the data when five assumptions can be made.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Linear Relation</strong>: the true regression relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is linear.</p></li>
<li><p><strong>Normal Errors</strong>: the error terms <span class="math inline">\(\epsilon_i\)</span> are normally distributed with a mean of zero.</p></li>
<li><p><strong>Constant Variance</strong>: the variance <span class="math inline">\(\sigma^2\)</span> of the error terms is constant (the same) over all <span class="math inline">\(X_i\)</span> values.</p></li>
<li><p><strong>Fixed X</strong>: the <span class="math inline">\(X_i\)</span> values can be considered fixed and measured without error.</p></li>
<li><p><strong>Independent Errors</strong>: the error terms <span class="math inline">\(\epsilon_i\)</span> are independent.</p></li>
</ol>
<div style="font-size:0.8em;">
<p>Note: see the <strong>Explanation</strong> tab for details about checking the regression assumptions.</p>
</div>
<p><strong>Interpretation</strong></p>
<p>The slope is interpreted as, “the change in the average y-value for a one unit change in the x-value.” It <strong>is not</strong> the average change in y. <strong>It is</strong> the change in the average y-value.</p>
<p>The y-intercept is interpreted as, “the average y-value when x is zero.” It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful.</p>
<hr />
</div>
</div>
<div id="r-instructions" class="section level3">
<h3>R Instructions</h3>
<div style="padding-left:125px;">
<p><strong>Console</strong> Help Command: <code>?lm()</code></p>
<p><strong>Perform the Regression</strong></p>
<p><code>mylm &lt;- lm(Y ~ X, data=YourDataSet)</code></p>
<p><code>summary(mylm)</code></p>
<ul>
<li><code>mylm</code> is some name you come up with to store the results of the <code>lm()</code> test. Note that <code>lm()</code> stands for “linear model.”</li>
<li><code>Y</code> must be a “numeric” vector of the quantitative response variable.</li>
<li><code>X</code> is the explanatory variable. It can either be quantitative (most usual) or qualitative.</li>
<li><code>YourDataSet</code> is the name of your data set.</li>
</ul>
<p><strong>Check Assumptions 1, 2, 3, and 5</strong></p>
<p><code>par(mfrow=c(1,3))</code></p>
<p><code>plot(mylm, which=1:2)</code></p>
<p><code>plot(mylm$residuals)</code></p>
<p><strong>Plotting the Regression Line</strong></p>
<p>To add the regression line to a scatterplot use the <code>abline(...)</code> command:</p>
<p><code>plot(Y ~ X, data=YourDataSet)</code></p>
<p><code>abline(mylm)</code></p>
<p>You can customize the look of the regression line with</p>
<p><code>abline(mylm, lty=1, lwd=1, col=&quot;someColor&quot;)</code></p>
<p>where <code>lty</code> stands for line-type with options (0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash) and <code>lwd</code> stands for line-width.</p>
<p><strong>Accessing Parts of the Regression</strong></p>
<p>Finally, note that the <code>mylm</code> object contains the <code>names(mylm)</code> of</p>
<ul>
<li><code>mylm$coefficients</code> Contains two values. The first is the estimated <span class="math inline">\(y\)</span>-intercept. The second is the estimated slope.</li>
<li><code>mylm$residuals</code> Contains the residuals from the regression in the same order as the actual dataset.</li>
<li><code>mylm$fitted.values</code> The values of <span class="math inline">\(\hat{Y}\)</span> in the same order as the original dataset.</li>
<li><code>mylm$...</code> several other things that will not be explained here.</li>
</ul>
<hr />
</div>
</div>
<div id="explanation" class="section level3">
<h3>Explanation</h3>
<div style="padding-left:125px;">

<p>Linear regression has a rich mathematical theory behind it. This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable <span class="math inline">\(Y\)</span> and an explanatory variable called <span class="math inline">\(X\)</span>.</p>
<div style="padding-left:30px;color:darkgray;">
<p>Expand each element below to learn more.</p>
</div>
<p><span style="color:steelblue;font-size:.8em;padding-left:160px;">Regression Cheat Sheet</span> <a href="javascript:showhide('regressioncheatsheet')" style="font-size:.6em;color:skyblue;">(Expand)</a></p>
<div id="regressioncheatsheet" style="display:none;font-size:.7em;">
<table style="width:71%;">
<colgroup>
<col width="9%" />
<col width="23%" />
<col width="11%" />
<col width="12%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Pronunciation</th>
<th>Meaning</th>
<th>Math</th>
<th>R Code</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Y_i\)</span></td>
<td>“why-eye”</td>
<td>The data</td>
<td><span class="math inline">\(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \sigma^2)\)</span></td>
<td><code>YourDataSet$YourYvariable</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{Y}_i\)</span></td>
<td>“why-hat-eye”</td>
<td>The fitted line</td>
<td><span class="math inline">\(\hat{Y}_i = b_0 + b_1 X_i\)</span></td>
<td><code>lmObject$fitted.values</code></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(E\{Y_i\}\)</span></td>
<td>“expected value of why-eye”</td>
<td>True mean y-value</td>
<td><span class="math inline">\(E\{Y_i\} = \beta_0 + \beta_1 X_i\)</span></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_0\)</span></td>
<td>“beta-zero”</td>
<td>True y-intercept</td>
<td><code>&lt;none&gt;</code></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_1\)</span></td>
<td>“beta-one”</td>
<td>True slope</td>
<td><code>&lt;none&gt;</code></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(b_0\)</span></td>
<td>“b-zero”</td>
<td>Estimated y-intercept</td>
<td><span class="math inline">\(b_0 = \bar{Y} - b_1\bar{X}\)</span></td>
<td><code>b_0 &lt;- mean(Y) - b_1*mean(X)</code></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(b_1\)</span></td>
<td>“b-one”</td>
<td>Estimated slope</td>
<td><span class="math inline">\(b_1 = \frac{\sum X_i(Y_i - \bar{Y})}{\sum(X_i - \bar{X})^2}\)</span></td>
<td><code>b_1 &lt;- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 )</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\epsilon_i\)</span></td>
<td>“epsilon-eye”</td>
<td>Distance of dot to true line</td>
<td><span class="math inline">\(\epsilon_i = Y_i - E\{Y_i\}\)</span></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(r_i\)</span></td>
<td>“r-eye” or “residual-eye”</td>
<td>Distance of dot to estimated line</td>
<td><span class="math inline">\(r_i = Y_i - \hat{Y}_i\)</span></td>
<td><code>lmObject$residuals</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sigma^2\)</span></td>
<td>“sigma-squared”</td>
<td>Variance of the <span class="math inline">\(\epsilon_i\)</span></td>
<td><span class="math inline">\(Var\{\epsilon_i\} = \sigma^2\)</span></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(MSE\)</span></td>
<td>“mean squared error”</td>
<td>Estimate of <span class="math inline">\(\sigma^2\)</span></td>
<td><span class="math inline">\(MSE = \frac{SSE}{n-p}\)</span></td>
<td><code>sum( lmObject$res^2 ) / (n - p)</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(SSE\)</span></td>
<td>“sum of squared error” (residuals)</td>
<td>Measure of dot’s total deviation from the line</td>
<td><span class="math inline">\(SSE = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2\)</span></td>
<td><code>sum( lmObject$res^2 )</code></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(SSR\)</span></td>
<td>“sum of squared regression error”</td>
<td>Measure of line’s deviation from y-bar</td>
<td><span class="math inline">\(SSR = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2\)</span></td>
<td><code>sum( (lmObject$fit - mean(YourData$Y))^2 )</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(SSTO\)</span></td>
<td>“total sum of squares”</td>
<td>Measure of total variation in Y</td>
<td><span class="math inline">\(SSR + SSE = SSTO = \sum_{i=1}^n (Y_i - \bar{Y})^2\)</span></td>
<td><code>sum( (YourData$Y - mean(YourData$Y)^2 )</code></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(R^2\)</span></td>
<td>“R-squared”</td>
<td>Proportion of variation in Y explained by the regression</td>
<td><span class="math inline">\(R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}\)</span></td>
<td><code>SSR/SSTO</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{Y}_h\)</span></td>
<td>“why-hat-aitch”</td>
<td>Estimated mean y-value for some x-value called <span class="math inline">\(X_h\)</span></td>
<td><span class="math inline">\(\hat{Y}_h = b_0 + b_1 X_h\)</span></td>
<td><code>predict(lmObject, data.frame(XvarName=#))</code></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(X_h\)</span></td>
<td>“ex-aitch”</td>
<td>Some x-value, not necessarily one of the <span class="math inline">\(X_i\)</span> values used in the regression</td>
<td>$X_h = $ some number</td>
<td><code>Xh = #</code></td>
</tr>
</tbody>
</table>
</div>
<p><br /></p>
<div id="the-mathematical-model-expand" class="section level4">
<h4>The Mathematical Model <a href="javascript:showhide('mathmodel1')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span class="math inline">\(Y_i\)</span>, <span class="math inline">\(\hat{Y}_i\)</span>, and <span class="math inline">\(E\{Y_i\}\)</span>…</span></p>
<div id="mathmodel1" style="display:none;">
<p>There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the “Regression Relation Diagram.”</p>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>Study both the three bullet points and their visual representations in the plot below for a clearer understanding.</p>
</div>
<ol style="list-style-type: decimal">
<li>The <strong>true line</strong>, i.e., the regression relation:</li>
</ol>
<div style="padding-left:60px;color:darkgray;">
<div style="color:steelblue;">
<p><span class="math inline">\(\underbrace{E\{Y\}}_{\substack{\text{true mean} \\ \text{y-value}}} = \underbrace{\overbrace{\beta_0}^\text{y-intercept} + \overbrace{\beta_1}^\text{slope} X}_\text{equation of a line}\)</span></p>
</div>
<p><a href="javascript:showhide('readmoretrueline')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="readmoretrueline" style="display:none;">
<p>The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as “natural law” or “God’s law”. It is some true line that is unknown to us.</p>
<p>The regression relation <span class="math inline">\(E\{Y\} = \beta_0 + \beta_1 X\)</span> creates the line of regression where <span class="math inline">\(\beta_0\)</span> is the <span class="math inline">\(y\)</span>-intercept of the line and <span class="math inline">\(\beta_1\)</span> is the slope of the line. The regression relationship provides the average <span class="math inline">\(Y\)</span>-value, denoted <span class="math inline">\(E\{Y_i\}\)</span>, for a given <span class="math inline">\(X\)</span>-value, denoted by <span class="math inline">\(X_i\)</span>.</p>
<p>Note: <span class="math inline">\(E\{Y\}\)</span> is pronounced “the expected value of y” because, well… the mean is the typical, average, or “expected” value.</p>
</div>
</div>
<ol start="2" style="list-style-type: decimal">
<li>The <strong>dots</strong>, i.e., the regression relation plus an error term:</li>
</ol>
<div style="padding-left:60px;color:darkgray;">
<div style="color:steelblue;">
<p><span class="math inline">\(Y_i = \underbrace{\beta_0 + \beta_1 X_i}_{E\{Y_i\}} + \underbrace{\epsilon_i}_\text{error term} \quad \text{where} \ \epsilon_i\sim N(0,\sigma^2)\)</span></p>
</div>
<p><a href="javascript:showhide('readmoredots')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="readmoredots" style="display:none;">
<p>This is shown by the dots in the graph below. This is the data. In regression, the assumtion is that the y-value for individual <span class="math inline">\(i\)</span>, denoted by <span class="math inline">\(Y_i\)</span>, was “created” by adding an error term <span class="math inline">\(\epsilon_i\)</span> to each individual’s “expected” value <span class="math inline">\(\beta_0 + \beta_1 X_i\)</span>. Note the “order of creation” would require first knowing an indivual’s x-value, <span class="math inline">\(X_i\)</span>, then their expected value from the regression relation <span class="math inline">\(E\{Y_i\} = \beta_0 + \beta_1 X_i\)</span> and then adding their <span class="math inline">\(\epsilon_i\)</span> value to the result. The <span class="math inline">\(\epsilon_i\)</span> allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance <span class="math inline">\(\epsilon_i\)</span> from the line.</p>
<p>Note: <span class="math inline">\(Y_i\)</span> is pronounced “why-eye” because it is the y-value for individual <span class="math inline">\(i\)</span>. Sometimes also called “why-sub-eye” because <span class="math inline">\(i\)</span> is in the subscript of <span class="math inline">\(Y\)</span>.</p>
</div>
</div>
<ol start="3" style="list-style-type: decimal">
<li>The <strong>estimated line</strong>, i.e., the line we get from a sample of data.</li>
</ol>
<div style="padding-left:60px;color:darkgray;">
<div style="color:steelblue;">
<p><span class="math inline">\(\underbrace{\hat{Y}_i}_{\substack{\text{estimated mean} \\ \text{y-value}}} = \underbrace{b_0 + b_1 X_i}_\text{estimated regression equation}\)</span></p>
</div>
<p><a href="javascript:showhide('readmoreestimatedline')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="readmoreestimatedline" style="display:none;">
<p>The estimated line is shown by the solid line in the graph below. <span class="math inline">\(\hat{Y}\)</span> is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation <span class="math inline">\(E\{Y\}\)</span>. So <span class="math inline">\(\hat{Y}\)</span> is interpreted as the estimated average (or mean) <span class="math inline">\(Y\)</span>-value for any given <span class="math inline">\(X\)</span>-value. Thus, <span class="math inline">\(b_0\)</span> is the estimated y-intercept and <span class="math inline">\(b_1\)</span> is the estimated slope. The b’s are sample statistics, like <span class="math inline">\(\bar{x}\)</span> and the <span class="math inline">\(\beta\)</span>’s are population parameters like <span class="math inline">\(\mu\)</span>. The <span class="math inline">\(b\)</span>’s estimate the <span class="math inline">\(\beta\)</span>’s.</p>
<p>Note: <span class="math inline">\(\hat{Y}_i\)</span> is pronounced “why-hat-eye” and is known as the “estimated y-value” or “fitted y-value” because it is the y-value you get from <span class="math inline">\(b_0 + b_1 X_i\)</span>. It is always different from <span class="math inline">\(Y_i\)</span> because dots are rarely if ever exactly on the estimated regression line.</p>
</div>
</div>
<p>This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line).</p>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Something to ponder: The true line, when coupled with the error terms, “creates” the data. The estimated (or fitted) line uses the sampled data to try to “re-create” the true line.</p>
<pre class="r"><code>## Simulating Data from a Regression Model
## This R-chunk is meant to be played in your R Console.
## It allows you to explore how the various elements
## of the regression model combine together to &quot;create&quot;
## data and then use the data to &quot;re-create&quot; the line.

set.seed(101) #Allows us to always get the same &quot;random&quot; sample
              #Change to a new number to get a new sample

  n &lt;- 30 #set the sample size

  X_i &lt;- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45.

  beta0 &lt;- 3 #Our choice for the y-intercept. 

  beta1 &lt;- 1.8 #Our choice for the slope. 

  sigma &lt;- 2.5 #Our choice for the std. deviation of the error terms.

  epsilon_i &lt;- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.

  Y_i &lt;- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model

  fabData &lt;- data.frame(y=Y_i, x=X_i) #Store the data as data

  View(fabData) 
  
  #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it.

  fab.lm &lt;- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData.

  summary(fab.lm) #Summarize your model. 

  plot(y ~ x, data=fabData) #Plot the data.

  abline(fab.lm) #Add the estimated regression line to your plot.

# Now for something you can&#39;t do in real life... but since we created the data...

  abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). 

  legend(&quot;topleft&quot;, legend=c(&quot;True Line&quot;, &quot;Estimated Line&quot;), lty=c(2,1), bty=&quot;n&quot;) #Add a legend to your plot specifying which line is which.</code></pre>
</div>
<p><br /></p>
</div>
<div id="interpreting-the-model-parameters-expand" class="section level4">
<h4>Interpreting the Model Parameters <a href="javascript:showhide('interpretingparameters')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span class="math inline">\(\beta_0\)</span> (intercept) and <span class="math inline">\(\beta_1\)</span> (slope), estimated by <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, interpreted as…</span></p>
<div id="interpretingparameters" style="display:none;">
<p>The interpretation of <span class="math inline">\(\beta_0\)</span> is only meaningful if <span class="math inline">\(X=0\)</span> is in the scope of the model. If <span class="math inline">\(X=0\)</span> is in the scope of the model, then the intercept is interpreted as the average y-value, denoted <span class="math inline">\(E\{Y\}\)</span>, when <span class="math inline">\(X=0\)</span>.</p>
<p>The interpretation of <span class="math inline">\(\beta_1\)</span> is the amount of increase (or decrease) in the average y-value, denoted <span class="math inline">\(E\{Y\}\)</span>, per unit change in <span class="math inline">\(X\)</span>. It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”.</p>
<p>To better see this, consider the three graphics shown below.</p>
<pre class="r"><code>par(mfrow=c(1,3))
hist(mtcars$mpg, main=&quot;Gas Mileage of mtcars Vehicles&quot;, ylab=&quot;Number of Vehicles&quot;, xlab=&quot;Gas Mileage (mpg)&quot;, col=&quot;skyblue&quot;)
boxplot(mpg ~ cyl, data=mtcars, border=&quot;skyblue&quot;, boxwex=0.5, main=&quot;Gas Mileage of mtcars Vehicles&quot;, ylab=&quot;Gas Mileage (mpg)&quot;, xlab=&quot;Number of Cylinders of Engine (cyl)&quot;)
plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=&quot;skyblue&quot;, main=&quot;Gas Mileage of mtcars Vehicles&quot;, ylab=&quot;Gas Mileage (mpg)&quot;, xlab=&quot;Quarter Mile Time (qsec)&quot;)
abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=&quot;darkgray&quot;)
mtext(side=3, text=&quot;Automatic Transmissions Only (am==0)&quot;, cex=0.5)
abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=&quot;gray&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<table style="width:88%;">
<colgroup>
<col width="27%" />
<col width="27%" />
<col width="31%" />
</colgroup>
<thead>
<tr class="header">
<th>The Histogram</th>
<th>The Boxplot</th>
<th>The Scatterplot</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>The <strong>histogram</strong> on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09.</td>
<td>The <strong>boxplot</strong> in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by <span class="math inline">\(26.66 - 19.74 = 6.92\)</span> mpg, then by <span class="math inline">\(19.74 - 15.1 = 4.64\)</span> mpg.</td>
<td>The <strong>scatterplot</strong> on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value.</td>
</tr>
</tbody>
</table>
</div>
<p><br /></p>
</div>
<div id="residuals-and-errors-expand" class="section level4">
<h4>Residuals and Errors <a href="javascript:showhide('residualsanderrors')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span class="math inline">\(r_i\)</span>, the residual, estimates <span class="math inline">\(\epsilon_i\)</span>, the true error…</span></p>
<div id="residualsanderrors" style="display:none;">
<p>Residuals are the difference between the observed value of <span class="math inline">\(Y_i\)</span> (the point) and the predicted, or estimated value, for that point called <span class="math inline">\(\hat{Y_i}\)</span>. The errors are the true distances between the observed <span class="math inline">\(Y_i\)</span> and the actual regression relation for that point, <span class="math inline">\(E\{Y_i\}\)</span>.</p>
<p>We will denote a <strong>residual</strong> for individual <span class="math inline">\(i\)</span> by <span class="math inline">\(r_i\)</span>, <span class="math display">\[
  r_i = \underbrace{Y_i}_{\substack{\text{Observed} \\ \text{Y-value}}} - \underbrace{\hat{Y}_i}_{\substack{\text{Predicted} \\ \text{Y-value}}} \quad \text{(residual)}
\]</span> The residual <span class="math inline">\(r_i\)</span> estimates the true <strong>error</strong> for individual <span class="math inline">\(i\)</span>, <span class="math inline">\(\epsilon_i\)</span>, <span class="math display">\[
  \epsilon_i = \underbrace{Y_i}_{\substack{\text{Observed} \\ \text{Y-value}}} - \underbrace{E\{Y_i\}}_{\substack{\text{True Mean} \\ \text{Y-value}}} \quad \text{(error)}
\]</span></p>
<p>In summary…</p>
<div style="padding-left:30px;">
<table style="width:53%;">
<colgroup>
<col width="23%" />
<col width="29%" />
</colgroup>
<thead>
<tr class="header">
<th>Residual <span class="math inline">\(r_i\)</span></th>
<th>Error <span class="math inline">\(\epsilon_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Distance between the dot <span class="math inline">\(Y_i\)</span> and the estimated line <span class="math inline">\(\hat{Y}_i\)</span></td>
<td>Distance between the dot <span class="math inline">\(Y_i\)</span> and the true line <span class="math inline">\(E\{Y_i\}\)</span>.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(r_i = Y_i - \hat{Y}_i\)</span></td>
<td><span class="math inline">\(\epsilon_i = Y_i - E\{Y_i\}\)</span></td>
</tr>
<tr class="odd">
<td>Known</td>
<td>Typically Unknown</td>
</tr>
</tbody>
</table>
</div>
<p>As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms.</p>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Keep in mind the idea that the errors <span class="math inline">\(\epsilon_i\)</span> “created” the data and that the residuals <span class="math inline">\(r_i\)</span> are computed after using the data to “re-create” the line.</p>
<p>Residuals have many uses in regression analysis. They allow us to</p>
<ol style="list-style-type: decimal">
<li>diagnose the regression assumptions,</li>
</ol>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>See the “Assumptions” section below for more details.</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>estimate the regression relation,</li>
</ol>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>See the “Estimating the Model Parameters” section below for more details.</p>
</div>
<ol start="3" style="list-style-type: decimal">
<li>estimate the variance of the error terms,</li>
</ol>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>See the “Estimating the Model Variance” section below for more details.</p>
</div>
<ol start="4" style="list-style-type: decimal">
<li>and assess the fit of the regression relation.</li>
</ol>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>See the “Assessing the Fit of a Regression” section below for more details.</p>
</div>
</div>
<p><br /></p>
</div>
<div id="residual-plots-regression-assumptions-expand" class="section level4">
<h4>Residual Plots &amp; Regression Assumptions <a href="javascript:showhide('assumptions1')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">Residuals vs. fitted-values, Q-Q Plot of the residuals, and residuals vs. order plots…</span></p>
<div id="assumptions1" style="display:none;">
<p>There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate.</p>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>Each assumption is labeled in the regression equation below.</p>
</div>
<ol style="list-style-type: decimal">
<li>The regression relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is linear.</li>
<li>The error terms are normally distributed with <span class="math inline">\(E\{\epsilon_i\}=0\)</span>.</li>
<li>The variance of the error terms is constant over all <span class="math inline">\(X\)</span> values.</li>
<li>The <span class="math inline">\(X\)</span> values can be considered fixed and measured without error.</li>
<li>The error terms are independent.</li>
</ol>
<p><span style="color:darkgray;">Regression Equation</span> <span class="math display">\[
  Y_i = \underbrace{\beta_0 + \beta_1 \overbrace{X_i}^\text{#4}}_{\text{#1}} + \epsilon_i \quad \text{where} \ \overbrace{\epsilon_i \sim}^\text{#5} \overbrace{N(0}^\text{#2}, \overbrace{\sigma^2}^\text{#3})
\]</span></p>
<p>Residuals are used to diagnose departures from the regression assumptions.</p>
<p><a href="javascript:showhide('moreassumptionsdetail')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="moreassumptionsdetail" style="display:none;">
<p>As shown above, the regression equation makes several claims, or assumptions, about the error terms <span class="math inline">\(\epsilon_i\)</span>, specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span> as shown here <span class="math display">\[
  \epsilon_i \underbrace{\sim}_{\substack{\text{Independent} \\ \text{Errors}}} \overbrace{N}^{\substack{\text{Normally} \\ \text{distributed}}}(\underbrace{0}_{\substack{\text{mean of} \\ \text{zero}}}, \underbrace{\sigma^2}_{\substack{\text{Constant} \\ \text{Variance}}})
\]</span></p>
<p>While the actual error terms (<span class="math inline">\(\epsilon_i\)</span>) are unknown in real life, the residuals (<span class="math inline">\(r_i\)</span>) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not.</p>
</div>
<p><br /></p>
<div style="padding-left:15px;">
<h5 id="residuals-versus-fitted-values-plot-checks-assumptions-1-and-3">Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3</h5>
<table width="90%">
<tr>
<td with="15%">
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-5-1.png" width="144" /></p>
</td>
<td width="75%">
<p>The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the <span class="math inline">\(\hat{Y}_i\)</span>. The residuals are the <span class="math inline">\(r_i\)</span>. This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable.</p>
<p>| <a href="javascript:showhide('residualsvsfittedvalues')" style="font-size:.8em;color:steelblue2;">Show Examples</a> |</p>
</td>
</tr>
</table>
<div id="residualsvsfittedvalues" style="display:none;">
<p><a href="javascript:showhide('residualsvsfittedvaluesread')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="residualsvsfittedvaluesread" style="display:none;">
<p>The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption.</p>
<ul>
<li><p>The linear relation is assumed to be satisfied if there are no apparent trends in the plot.</p></li>
<li><p>The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values.</p></li>
</ul>
<p>The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance.</p>
</div>
<pre class="r"><code>set.seed(2)
X &lt;- rnorm(30,15,3)
notLin &lt;- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8))
notLin.lm &lt;- lm(Y~X, data=notLin)
set.seed(15)
Lin &lt;- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3))
Lin.lm &lt;- lm(Y~X, data=Lin)
par(mfrow=c(3,3),  mai=c(.25,.25,.25,.25), mgp=c(1,.75,0))
  plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Not Linear&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  mycurve &lt;- lowess(notLin.lm$fitted.values,notLin.lm$residuals)
  mycurveOrder &lt;- order(mycurve$x)
  mycurve$x &lt;- mycurve$x[mycurveOrder]
  mycurve$y &lt;- mycurve$y[mycurveOrder]
  polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) 
  abline(h=0)
  plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, 
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Good: Linear, Constant Variance&quot;, 
       cex.main=0.95, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)
  abline(h=0)

  set.seed(6)
notCon &lt;- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5))
notCon.lm &lt;- lm(Y~X, data=notCon)
LinO &lt;- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3))
LinO[1] &lt;- LinO[1]^2
LinO.lm &lt;- lm(Y~X, data=LinO)
  plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, main=&quot;Unconstant Variance&quot;, cex.main=0.95, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) 
  abline(h=0)
#  plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, main=&quot;Outliers&quot;, cex.main=0.95)
#  abline(h=0)

  
  tmp &lt;- lm(height ~ age, data=Loblolly)
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  mycurve &lt;- lowess(tmp$fitted.values,tmp$residuals)
  mycurveOrder &lt;- order(mycurve$x)
  mycurve$x &lt;- mycurve$x[mycurveOrder]
  mycurve$y &lt;- mycurve$y[mycurveOrder]
  polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) 
  abline(h=0)
  
  tmp &lt;- lm(Girth ~ Volume, data=trees[-31,])
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)
  abline(h=0)

  tmp &lt;- lm(Height ~ Volume, data=trees)
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) 
  abline(h=0)
  
  
  
  
  tmp &lt;- lm(mpg ~ disp, data=mtcars)
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  mycurve &lt;- lowess(tmp$fitted.values,tmp$residuals, f=.4)
  mycurveOrder &lt;- order(mycurve$x)
  mycurve$x &lt;- mycurve$x[mycurveOrder]
  mycurve$y &lt;- mycurve$y[mycurveOrder]
  polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) 
  abline(h=0) 
  
  
  tmp &lt;- lm(weight ~ repwt, data=Davis[-12,])
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)
  abline(h=0) 

  tmp &lt;- lm(weight ~ repht, data=Davis[-12,])
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) 
  abline(h=0) </code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<p><br /></p>
<h5 id="q-q-plot-of-the-residuals-checks-assumption-2">Q-Q Plot of the Residuals: Checks Assumption #2</h5>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-7-1.png" width="144" />
</td>
<td width="75%">
<p>The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption.</p>
<p>| <a href="javascript:showhide('qqplots')" style="font-size:.8em;color:steelblue2;">Show Examples</a> |</p>
</td>
</tr>
</table>
<div id="qqplots" style="display:none;">
<p><a href="javascript:showhide('qqplotsread')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="qqplotsread" style="display:none;">
<p>There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot.</p>
<p>Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution.</p>
</div>
<pre class="r"><code>par(mfrow=c(2,2),  mai=c(.5,.5,.25,.25), mgp=c(1,.75,0))

set.seed(123)

  tmp &lt;- rnorm(100)
  qqnorm(tmp, pch=20, ylab=&quot;Observed&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)
  qqline(tmp)
  hist(tmp, xlab=&quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;Normal&quot;, col=&quot;skyblue&quot;)
  
  tmp &lt;- Davis$weight
  qqnorm(tmp, pch=20, ylab=&quot;Observed&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  qqline(tmp)
  hist(tmp, xlab=&quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;Right-skewed&quot;,
       breaks=15, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(2,2),  mai=c(.5,.5,.25,.25), mgp=c(1,.75,0))

  tmp &lt;- rbeta(100, 5,1)
  qqnorm(tmp, pch=20, ylab=&quot;Observed&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  qqline(tmp)
  hist(tmp, xlab=&quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;Left-skewed&quot;,
       breaks=seq(min(tmp),max(tmp), length.out=13), col=&quot;firebrick&quot;)
  
  tmp &lt;- rbeta(100,2,2)
  qqnorm(tmp, pch=20, ylab=&quot;Observed&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  qqline(tmp)
  hist(tmp, xlab=&quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;Heavy-tailed&quot;, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
</div>
<p><br /></p>
<h5 id="residuals-versus-order-plot-checks-assumption-5">Residuals versus Order Plot: Checks Assumption #5</h5>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-9-1.png" width="144" />
</td>
<td width="75%">
<p>When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated.</p>
<p>| <a href="javascript:showhide('resorderplots')" style="font-size:.8em;color:steelblue2;">Show Examples</a> |</p>
</td>
</tr>
</table>
<div id="resorderplots" style="display:none;">
<p><a href="javascript:showhide('resorderplotsread')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="resorderplotsread" style="display:none;">
<p>Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated.</p>
</div>
<pre class="r"><code>par(mfrow=c(2,2),  mai=c(.5,.5,.25,.25), mgp=c(1,.75,0))

  tmp &lt;- lm(mpg ~ disp, data=mtcars)
  plot(tmp$residuals, pch=20,
       xlab=&quot;Order&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Good: No Trend&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)

  tmp &lt;- lm(height ~ age, data=Loblolly)
  plot(tmp$residuals, pch=20,
       xlab=&quot;Order&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Questionable: General Trend&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;orangered&quot;)

  tmp &lt;- lm(hp ~ qsec, data=mtcars)
  plot(tmp$residuals, pch=20,
       xlab=&quot;Order&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Questionable: Interesting Patterns&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;orangered&quot;)
  
  tmp &lt;- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),])
  plot(tmp$residuals, pch=20,
       xlab=&quot;Order&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Bad: Obvious Trend&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
</div>
<p><br /></p>
</div>
<p><br /></p>
</div>
<div id="assessing-the-fit-of-a-regression-expand" class="section level4">
<h4>Assessing the Fit of a Regression <a href="javascript:showhide('assessingthefit')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span class="math inline">\(R^2\)</span>, SSTO, SSR, and SSE…</span></p>
<div id="assessingthefit" style="display:none;">

<p>Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line.</p>
<pre class="r"><code>par(mfrow=c(1,3), mai=c(.1,.1,.5,.1))
set.seed(2)
x &lt;- runif(30,0,20)
y1 &lt;- 2 + 3.5*x + rnorm(30,0,2)
y2 &lt;- 2 + 3.5*x + rnorm(30,0,8)
y3 &lt;- 2 + 3.5*x + rnorm(30,0,27)
plot(y1 ~ x, pch=16, col=&quot;darkgray&quot;, xlim=c(-1,21), yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ylim=c(-10,100), main=&quot;Excellent Fit&quot;)
abline(lm(y1 ~ x), col=&quot;gray&quot;)
plot(y2 ~ x, pch=16, col=&quot;darkgray&quot;, xlim=c(-1,21), yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ylim=c(-10,100), main=&quot;Good Fit&quot;)
abline(lm(y2 ~ x), col=&quot;gray&quot;)
plot(y3 ~ x, pch=16, col=&quot;darkgray&quot;, xlim=c(-1,21), yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ylim=c(-10,100), main=&quot;Poor Fit&quot;)
abline(lm(y3 ~ x), col=&quot;gray&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>A common way to measure the fit of a regression is with <a href="NumericalSummaries.html#correlation">correlation</a>. While this can be a useful measurement, there is greater insight in using the square of the correlation, called <span class="math inline">\(R^2\)</span>. Before you can understand <span class="math inline">\(R^2\)</span>, you must understand three important “sums of squares”.</p>
<div style="padding-left:30px;">
<p><a href="javascript:showhide('sumsofsquaresread')" style="font-size:.8em;color:skyblue;">(Read more about sums…)</a></p>
<div id="sumsofsquaresread" style="display:none;">
<p>A sum is just a fancy word for adding things together. <span class="math display">\[
  1 + 2 + 3 + 4 + 5 + 6 = 21
\]</span></p>
<p>Long sums get tedious to write out by hand. So we use the symbol <span class="math inline">\(\Sigma\)</span> to denote the word “sum”. Further, we use a subscript <span class="math inline">\(\Sigma_{i=1}\)</span> to state what value the sum is beginning with, and a superscript <span class="math inline">\(\Sigma_{i=1}^6\)</span> to state the value we are ending at. This gives <span class="math display">\[
  \sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21
\]</span></p>
<p>Test your knowledge, do you see why the answer is 6 to the sum below? <span class="math display">\[
  \sum_{i=1}^3 i = 6
\]</span></p>
<p>Computing sums in R is fairly easy. Type the following codes in your R Console.</p>
<p><code>sum(1:6) #gives the answer of 21</code></p>
<p><code>sum(1:3) #gives the answer of 6</code></p>
<p>However, sums really become useful when used with a data set.</p>
<p>Each row of a data set represents an “individual’s” data. We can reference each individual with a row number. In the data below, individual 3, denoted by <span class="math inline">\(i=3\)</span>, has a <code>speed</code> of 7 and a <code>dist</code> of 3.</p>
<pre class="r"><code>pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3)</code></pre>
<table style="width:40%;">
<colgroup>
<col width="18%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Individual</th>
<th align="center">speed</th>
<th align="center">dist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">4</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">4</td>
<td align="center">10</td>
</tr>
<tr class="odd">
<td align="center"><strong>3</strong></td>
<td align="center"><strong>7</strong></td>
<td align="center"><strong>4</strong></td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">7</td>
<td align="center">22</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">8</td>
<td align="center">16</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">9</td>
<td align="center">10</td>
</tr>
</tbody>
</table>
<p>To compute the sum of the <strong>speed</strong> column, use <code>sum(speed)</code>. If we divided this sum by 6, we would get the mean of speed <code>mean(speed)</code>. In fact, the two most used statistics <code>mean(...)</code> and <code>sd(...)</code> both use sums. Take a moment to review the formulas for <a href="NumericalSummaries.html#mean">mean</a> and <a href="NumericalSummaries.html#standard-deviation">standard deviation</a>. It is strongly recommended that you study the Explanation tab for both as well. We’ll wait. See you back here shortly.</p>
<p>…</p>
<p>Welcome back.</p>
<p>Suppose we let <code>X = speed</code> and <code>Y = dist</code>. Then <span class="math inline">\(X_3 = 7\)</span> and <span class="math inline">\(Y_3 = 4\)</span> because we are accessing row 3 of both the <span class="math inline">\(X\)</span> (or speed) column and <span class="math inline">\(Y\)</span> (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, <code>sum(speed)</code> would be written mathematically as <span class="math inline">\(\sum_{i=1}^6 X_i\)</span> and <code>sum(dist)</code> would be written as <span class="math inline">\(\sum_{i=1}^6 X_i\)</span>.</p>
</div>
</div>
<table>
<colgroup>
<col width="32%" />
<col width="36%" />
<col width="31%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Sum of Squared Errors</strong></th>
<th><strong>Sum of Squares Regression</strong></th>
<th><strong>Total Sum of Squares</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\text{SSE} = \sum_{i=1}^n \left(Y_i - \hat{Y}_i\right)^2\)</span></td>
<td><span class="math inline">\(\text{SSR} = \sum_{i=1}^n \left(\hat{Y}_i - \bar{Y}\right)^2\)</span></td>
<td><span class="math inline">\(\text{SSTO} = \sum_{i=1}^n \left(Y_i - \bar{Y}\right)^2\)</span></td>
</tr>
<tr class="even">
<td>Measures how much the residuals deviate from the line.</td>
<td>Measures how much the regression line deviates from the average y-value.</td>
<td>Measures how much the y-values deviate from the average y-value.</td>
</tr>
<tr class="odd">
<td>Equals SSTO - SSR</td>
<td>Equals SSTO - SSE</td>
<td>Equals SSE + SSR</td>
</tr>
<tr class="even">
<td><code>sum( (Y - mylm$fit)^2 )</code></td>
<td><code>sum( (mylm$fit - mean(Y))^2 )</code></td>
<td><code>sum( (Y - mean(Y))^2 )</code></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<hr style="border-color:#d5d5d5; border-style:solid;"/>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>It is important to remember that SSE and SSR split up SSTO, so that <span class="math display">\[
  \text{SSTO} = \text{SSE} + \text{SSR}
\]</span> This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works.</p>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called <span class="math inline">\(R^2\)</span> (“r-squared”).</p>
<p><strong>R-Squared (<span class="math inline">\(R^2\)</span>)</strong> <span class="math display">\[
  \underbrace{R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}}_\text{Interpretation: Proportion of variation in Y explained by the regression.}
\]</span></p>
<p>The smallest <span class="math inline">\(R^2\)</span> can be is zero, and the largest it can be is 1. This is because <span class="math inline">\(SSR\)</span> must be between 0 and SSTO, inclusive.</p>
</div>
<p><br /></p>
</div>
<div id="estimating-the-model-parameters-expand" class="section level4">
<h4>Estimating the Model Parameters <a href="javascript:showhide('estimatingparameters')" style="font-size:.6em;color:skyblue;" id="estMod">(Expand)</a></h4>
<p><span class="expand-caption">How to get <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>: least squares &amp; maximum likelihood…</span></p>
<div id="estimatingparameters" style="display:none;">
<p>There are two approaches to estimating the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> obtained from either method are identical. The estimates for the true parameter values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are typically denoted by <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, respectively, and are given by the following formulas.</p>
<table style="width:74%;">
<colgroup>
<col width="29%" />
<col width="31%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter Estimate</th>
<th>Mathematical Formula</th>
<th>R Code</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Slope</td>
<td><span class="math inline">\(b_1 = \frac{\sum X_i(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}\)</span></td>
<td><code>b_1 &lt;- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 )</code></td>
</tr>
<tr class="even">
<td>Intercept</td>
<td><span class="math inline">\(b_0 = \bar{Y} - b_1\bar{X}\)</span></td>
<td><code>b_0 &lt;- mean(Y) - b_1*mean(X)</code></td>
</tr>
</tbody>
</table>
It is important to note that these estimates are entirely determined from the observed data <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. When the regression equation is written using the estimates instead of the parameters, we use the notation <span class="math inline">\(\hat{Y}\)</span>, which is the estimator of <span class="math inline">\(E\{Y\}\)</span>. Thus, we write
<span class="math display">\[\begin{equation}
  \hat{Y}_i = b_0 + b_1 X_i
\end{equation}\]</span>
which is directly comparable to the true, but unknown values
<span class="math display">\[\begin{equation}
  E\{Y_i\} = \beta_0 + \beta_1 X_i. 
  \label{exp}
\end{equation}\]</span>
<h5 id="leastSquares">Least Squares</h5>
<p>To estimate the model parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> using least squares, we start by defining the function <span class="math inline">\(Q\)</span> as the sum of the squared errors, <span class="math inline">\(\epsilon_i\)</span>. <span class="math display">\[
  Q = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (Y_i - (\beta_0 + \beta_1 X_i))^2
\]</span> Then we use the function Q as if it were a function of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Ironically, the values of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we don’t know are the values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p>This <a href="https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_en.html">least squares applet</a> is a good way to explore how various choices of the slope and intercept yield different values of the “sum of squared residuals”. But it turns out that there is one “best” choice of the slope and intercept that yields a “smallest” value of the “sum of squared residuals.” This best choice can actually be found using calculus by taking the partial derivatives of <span class="math inline">\(Q\)</span> with respect to both <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. <span class="math display">\[
  \frac{\partial Q}{\partial \beta_0} = -2\sum (Y_i - \beta_0 - \beta_1X_i)
\]</span> <span class="math display">\[
  \frac{\partial Q}{\partial \beta_1} = -2\sum X_i(Y_i-\beta_0-\beta_1X_i)
\]</span> Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize <span class="math inline">\(Q\)</span> for a given set of data. After all the calculations are completed we find the values of the parameter estimators <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> (of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, respectively) are as stated previously.</p>
<h5 id="mle">Maximum Likelihood</h5>
<p>The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> which minime the least squares <span class="math inline">\(Q\)</span> function, we choose the values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the <span class="math inline">\(Y_i\)</span> for all observations <span class="math inline">\(i=1,\ldots,n\)</span>. We can do this rather simply by using the assumption that the errors, <span class="math inline">\(\epsilon_i\)</span> are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if <span class="math inline">\(f(Y_i)\)</span> denotes the probability density function for <span class="math inline">\(Y_i\)</span>, then the joint probability density for all <span class="math inline">\(Y_i\)</span>, <span class="math inline">\(f(Y_1,\ldots,Y_n)\)</span> is given by <span class="math display">\[
  f(Y_1,\ldots,Y_n) = \prod_{i=1}^n f(Y_i) 
\]</span> Since each <span class="math inline">\(Y_i\)</span> is assumed to be normally distributed with mean <span class="math inline">\(\beta_0 + \beta_1 X_i\)</span> and variance <span class="math inline">\(\sigma^2\)</span> (see model ()) we have that <span class="math display">\[
  f(Y_i) = \frac{1}{\sqrt{2\pi}\sigma}\exp{\left[-\frac{1}{2}\left(\frac{Y_i-\beta_0-\beta_1X_i}{\sigma}\right)^2\right]}
\]</span> which provides the joint probability as <span class="math display">\[
  f(Y_1,\ldots,Y_n) = \prod_{i=1}^n f(Y_i) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2\right]}
\]</span> The likelihood function <span class="math inline">\(L\)</span> is then given by consider the <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_i\)</span> fixed and the parameters <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\sigma^2\)</span> as the variables in the function. <span class="math display">\[
  L(\beta_0,\beta_1,\sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2\right]}
\]</span> Instead of taking partial derivatives of <span class="math inline">\(L\)</span> directly (with respect to all parameters) we take the partial derivatives of the <span class="math inline">\(\log\)</span> of <span class="math inline">\(L\)</span>, which is easier to work with. In a similar, but more difficult calculation, to that of minimizing <span class="math inline">\(Q\)</span>, we obtain the values of <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\sigma^2\)</span> which maximize the log of <span class="math inline">\(L\)</span>, and which therefore maximize <span class="math inline">\(L\)</span>. (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate <span class="math inline">\(\hat{\sigma}^2\)</span> of <span class="math inline">\(\sigma^2\)</span>. <span class="math display">\[
  \hat{\sigma}^2 = \frac{\sum(Y_i-\hat{Y}_i)^2}{n}
\]</span></p>
</div>
<p><br /></p>
</div>
<div id="estimating-the-model-variance-expand" class="section level4">
<h4>Estimating the Model Variance <a href="javascript:showhide('estimatingvariance')" style="font-size:.6em;color:skyblue;" id="varEst">(Expand)</a></h4>
<p><span class="expand-caption">Estimating <span class="math inline">\(\sigma^2\)</span> with MSE…</span></p>
<div id="estimatingvariance" style="display:none;">
As shown previously, we can obtain estimates for the model parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> with either least squares estimation or maximum likelihood estimation. It turns out that these estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. On the other hand, the maximum likelihood estimate <span class="math inline">\(\hat{\sigma}^2\)</span> of the model variance <span class="math inline">\(\sigma^2\)</span> is a biased estimator. It is consistently wrong in its estimates of <span class="math inline">\(\sigma^2\)</span>. Without going into all the details, <span class="math inline">\(\hat{\sigma}^2\)</span> is a biased estimator of <span class="math inline">\(\sigma^2\)</span> because its denominator needs to represent the degrees of freedom associated with the numerator. Since <span class="math inline">\(\hat{Y}_i\)</span> in the numerator of <span class="math inline">\(\hat{\sigma}^2\)</span> is defined by
<span class="math display">\[\begin{equation}
  \hat{Y}_i = b_0 + b_1X_i
  \label{hatY}
\end{equation}\]</span>
it follows that two means, <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\bar{Y}\)</span>, must be estimated from the data to obtain <span class="math inline">\(\hat{Y}_i\)</span>, see formulas () and () for details. Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for <span class="math inline">\(\hat{\sigma}^2\)</span> should be <span class="math inline">\(n-2\)</span> instead of <span class="math inline">\(n\)</span>. Some incredibly long calculations will show that the estimator
<span class="math display">\[\begin{equation}
  s^2 = MSE = \frac{\sum(Y_i-\hat{Y}_i)^2}{n-2}
\end{equation}\]</span>
is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. Here <span class="math inline">\(MSE\)</span> stands for mean squared error, which is the most obvious name for a formula that squares the errors <span class="math inline">\(Y_i-\hat{Y}_i\)</span> then adds them up and divides by their degrees of freedom. Similarly, we call the numerator <span class="math inline">\(\sum(Y_i-\hat{Y}_i)^2\)</span> the sum of the squared errors, denoted by <span class="math inline">\(SSE\)</span>. It is also important to note that the errors are often denoted by <span class="math inline">\(e_i = Y_i-\hat{Y}_i\)</span>. Putting this all together we get the following equivalent statements for <span class="math inline">\(MSE\)</span>.
<span class="math display">\[\begin{equation}
  s^2 = MSE = \frac{SSE}{n-2} = \frac{\sum(Y_i-\hat{Y}_i)^2}{n-2} = \frac{\sum e_i^2}{n-2}
\end{equation}\]</span>
<p>As a final note, even though the <span class="math inline">\(E\{MSE\} = \sigma^2\)</span>, <span class="math inline">\(MSE\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>, it unfortunately isn’t true that <span class="math inline">\(\sqrt{MSE}\)</span> is an unbiased estimator of <span class="math inline">\(\sigma\)</span>. This presents a few problems later on.</p>
</div>
<p><br /></p>
</div>
<div id="inference-for-the-model-parameters-expand" class="section level4">
<h4>Inference for the Model Parameters <a href="javascript:showhide('inference1')" style="font-size:.6em;color:skyblue;" id="infModelParam">(Expand)</a></h4>
<p><span class="expand-caption">t test formulas, sampling distributions, confidence intervals, and F tests…</span></p>
<div id="inference1" style="display:none;">
<p>Most inference in regression is focused on the slope, <span class="math inline">\(\beta_1\)</span>. Recall that the interpretation of <span class="math inline">\(\beta_1\)</span> is the amount of increase (or decrease) in the expected value of <span class="math inline">\(Y\)</span> per unit change in <span class="math inline">\(X\)</span>. There are three main scenarios where inference about the slope is of interest.</p>
<ol style="list-style-type: decimal">
<li><p>Determine if there is evidence of a meaningful linear relationship in the data. If <span class="math inline">\(\beta_1 = 0\)</span>, then there is no relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(E\{Y\}\)</span>. Hence we might be interested in testing the hypotheses <span class="math display">\[
  H_0: \beta_1 = 0
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq 0 
\]</span></p></li>
<li><p>Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form <span class="math display">\[
  H_0: \beta_1 = \beta_{10}
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10} 
\]</span> where <span class="math inline">\(\beta_{10}\)</span> is some hypothesized number.</p></li>
<li><p>To provide a confidence interval for the true value of <span class="math inline">\(\beta_1\)</span>.</p></li>
</ol>
<p><br /></p>
<p>Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the <strong>sampling distribution</strong> of the estimate <span class="math inline">\(b_1\)</span> of the parameter <span class="math inline">\(\beta_1\)</span>. And, while we are at it, we may as well come to understand the sampling distribution of the estimate <span class="math inline">\(b_0\)</span> of the parameter <span class="math inline">\(\beta_0\)</span>.</p>
<div style="padding-left:30px;color:darkgray;font-size:.8em;">
<p>Review <a href="http://statistics.byuimath.com/index.php?title=Lesson_6:_Distribution_of_Sample_Means_%26_The_Central_Limit_Theorem#Introduction_to_Sampling_Distributions">sampling distributions</a> from Math 221.</p>
</div>
<p>Since <span class="math inline">\(b_1\)</span> is an estimate, it will vary from sample to sample, even though the truth, <span class="math inline">\(\beta_1\)</span>, remains fixed. (The same holds for <span class="math inline">\(b_0\)</span> and <span class="math inline">\(\beta_0\)</span>.) It turns out that the sampling distribution of <span class="math inline">\(b_1\)</span> (where the <span class="math inline">\(X\)</span> values remain fixed from study to study) is normal with mean and variance: <span class="math display">\[
  \mu_{b_1} = \beta_1
\]</span> <span class="math display">\[
  \sigma^2_{b_1} = \frac{\sigma^2}{\sum(X_i-\bar{X})^2}
\]</span></p>
<pre class="r"><code>## Simulation to Show relationship between Standard Errors

##-----------------------------------------------
## Edit anything in this area... 

n &lt;- 100 #sample size
Xstart &lt;- 30 #lower-bound for x-axis
Xstop &lt;- 100 #upper-bound for x-axis

beta_0 &lt;- 2 #choice of true y-intercept
beta_1 &lt;- 3.5 #choice of true slope
sigma &lt;- 13.8 #choice of st. deviation of error terms

## End of Editable area.
##-----------------------------------------------


# Create X, which will be used in the next R-chunk.
X &lt;- rep(seq(Xstart,Xstop, length.out=n/2), each=2) 

## After playing this chunk, play the next chunk as well.</code></pre>
<p>To see that this is true, consider the regression model with values specified for each parameter as follows.</p>
<p><span class="math display">\[
  Y_i = \overbrace{\beta_0}^{2} + \overbrace{\beta_1}^{3.5} X_i + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \overbrace{\sigma^2}^{\sigma=13.8})
\]</span></p>
<p>Using the equations above for <span class="math inline">\(\mu_{b_1}\)</span> and <span class="math inline">\(\sigma^2_{b_1}\)</span> we obtain that the mean of the sampling distribution of <span class="math inline">\(b_1\)</span> will be</p>
<p><span class="math inline">\(\mu_{b_1} = \beta_1 = 3.5\)</span></p>
<p>Further, we see that the variance of the sampling distribution of <span class="math inline">\(b_1\)</span> will be</p>
<p><span class="math inline">\(\sigma^2_{b_1} = \frac{\sigma^2}{\sum(X_i-\bar{X})^2} = \frac{13.8^2}{4.25\times 10^{4}}\)</span></p>
<p>Taking the square root of the variance, the standard deviation of the sampling distribution of <span class="math inline">\(b_1\)</span> will be</p>
<p><span class="math inline">\(\sigma_{b_1} = 0.067\)</span>.</p>
<p>That’s very nice. But to really believe it, let’s run a simulation ourselves. The “Code” below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_0\)</span> for that regression.</p>
<p>After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of <span class="math inline">\(b_0\)</span> is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of <span class="math inline">\(b_1\)</span> is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of <span class="math inline">\(\beta_0\)</span> or <span class="math inline">\(\beta_1\)</span>, respectively. Also, the “Std. Error” of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of <span class="math inline">\(\sigma_{b_0}\)</span> and <span class="math inline">\(\sigma_{b_1}\)</span>, respectively. Amazing!</p>
<pre class="r"><code>N &lt;- 5000 #number of times to pull a random sample
storage_b0 &lt;- storage_b1 &lt;- storage_rmse &lt;- rep(NA, N)
for (i in 1:N){
  Y &lt;- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model
  mylm &lt;- lm(Y ~ X)
  storage_b0[i] &lt;- coef(mylm)[1]
  storage_b1[i] &lt;- coef(mylm)[2]
  storage_rmse[i] &lt;- summary(mylm)$sigma
}


layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3))

Ystart &lt;- 0 #min(0,min(Y)) 
Ystop &lt;- 500 #max(max(Y), 0)
Yrange &lt;- Ystop - Ystart

plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), 
     ylim=c(Ystart, Ystop), pch=16, col=&quot;gray&quot;,
     main=&quot;Regression Lines from many Samples (gray lines) \n Plus Residual Standard Deviation Lines (green lines)&quot;)
text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1)
text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1)
text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1)


for (i in 1:N){
  abline(storage_b0[i], storage_b1[i], col=&quot;darkgray&quot;)  
}
abline(beta_0, beta_1, col=&quot;green&quot;, lwd=3)
abline(beta_0+sigma, beta_1, col=&quot;green&quot;, lwd=2)
abline(beta_0-sigma, beta_1, col=&quot;green&quot;, lwd=2)
abline(beta_0+2*sigma, beta_1, col=&quot;green&quot;, lwd=1)
abline(beta_0-2*sigma, beta_1, col=&quot;green&quot;, lwd=1)
abline(beta_0+3*sigma, beta_1, col=&quot;green&quot;, lwd=.5)
abline(beta_0-3*sigma, beta_1, col=&quot;green&quot;, lwd=.5)

par(mai=c(1,.6,.5,.01))

  addnorm &lt;- function(m,s, col=&quot;firebrick&quot;){
    curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2)
    lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col)
    lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col)
    lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col)
    lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col)
    lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col)
    lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col)
    lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col)
    legend(&quot;topleft&quot;, legend=paste(&quot;Std. Error = &quot;, round(s,3)), cex=0.7, bty=&quot;n&quot;)
  }

  h0 &lt;- hist(storage_b0, 
             col=&quot;skyblue3&quot;, 
             main=&quot;Sampling Distribution\n Y-intercept&quot;,
             xlab=expression(paste(&quot;Estimates of &quot;, beta[0], &quot; from each Sample&quot;)),
             freq=FALSE, yaxt=&#39;n&#39;, ylab=&quot;&quot;)
  m0 &lt;- mean(storage_b0)
  s0 &lt;- sd(storage_b0)
  addnorm(m0,s0, col=&quot;green&quot;)
  
  h1 &lt;- hist(storage_b1, 
             col=&quot;skyblue3&quot;, 
             main=&quot;Sampling Distribution\n Slope&quot;,
             xlab=expression(paste(&quot;Estimates of &quot;, beta[1], &quot; from each Sample&quot;)),
             freq=FALSE, yaxt=&#39;n&#39;, ylab=&quot;&quot;)
  m1 &lt;- mean(storage_b1)
  s1 &lt;- sd(storage_b1)
  addnorm(m1,s1, col=&quot;green&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-17-1.png" width="768" /></p>
<div style="padding-left:15px;">
<h5 id="tTests">t Tests</h5>
<p>Using the information above about the sampling distributions of <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_0\)</span>, an immediate choice of statistical test to test the hypotheses <span class="math display">\[
  H_0: \beta_1 = \beta_{10} 
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10} 
\]</span> where <span class="math inline">\(\beta_{10}\)</span> can be zero, or any other value, is a t test given by <span class="math display">\[
  t = \frac{b_1 - \beta_{10}}{s_{b_1}}
\]</span> where <span class="math inline">\(s^2_{b_1} = \frac{MSE}{\sum(X_i-\bar{X})^2}\)</span>. (You may want to review the section “Estimating the Model Variance” of this file to know where MSE came from.) With quite a bit of work it has been shown that <span class="math inline">\(t\)</span> is distributed as a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-2\)</span> degrees of freedom. The nearly identical test statistic for testing <span class="math display">\[
  H_0: \beta_0 = \beta_{00}
\]</span> <span class="math display">\[
  H_a: \beta_0 \neq \beta_{00} 
\]</span> is given by <span class="math display">\[
  t = \frac{b_0 - \beta_{00}}{s_{b_0}}
\]</span> where <span class="math inline">\(s^2_{b_0} = MSE\left[\frac{1}{n}+\frac{\bar{X}^2}{\sum(X_i-\bar{X})^2}\right]\)</span>. This version of <span class="math inline">\(t\)</span> has also been shown to be distributed as a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-2\)</span> degrees of freedom.</p>
<h5 id="confidence-intervals">Confidence Intervals</h5>
<p>Creating a confidence interval for either <span class="math inline">\(\beta_1\)</span> or <span class="math inline">\(\beta_0\)</span> follows immediately from these results using the formulas <span class="math display">\[
  b_1 \pm t^*_{n-2}\cdot s_{b_1}
\]</span> <span class="math display">\[
  b_0 \pm t^*_{n-2}\cdot s_{b_0}
\]</span> where <span class="math inline">\(t^*_{n-2}\)</span> is the critical value from a t distribution with <span class="math inline">\(n-2\)</span> degrees of freedom corresponding to the chosen confidence level.</p>
<p><br /></p>
<h5 id="Ftests">F tests</h5>
<p>Another way to test the hypotheses <span class="math display">\[
  H_0: \beta_1 = \beta_{10}  \quad\quad \text{or} \quad\quad H_0: \beta_0 = \beta_{00}
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10} \quad\quad \ \ \quad \quad H_a: \beta_0 \neq \beta_{00}
\]</span> is with an <span class="math inline">\(F\)</span> Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an <span class="math inline">\(F\)</span> test is very general and can be used in many places that a t test cannot.</p>
<p>In its most general form, the <span class="math inline">\(F\)</span> test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that <span class="math inline">\(H_0:\beta_1=0\)</span> against the alternative that <span class="math inline">\(H_a: \beta_1\neq 0\)</span>, we are essentially comparing two models against each other. If <span class="math inline">\(\beta_1=0\)</span>, then the corresponding model would be <span class="math inline">\(E\{Y_i\} = \beta_0\)</span>. If <span class="math inline">\(\beta_1\neq0\)</span>, then the model remains <span class="math inline">\(E\{Y_i\}=\beta_0+\beta_1X_i\)</span>. We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the <span class="math inline">\(F\)</span> Test, that the null model (reduced model) have fewer “free” parameters than the alternative model (full model). To demonstrate what we mean by “free” parameters, consider the following example.</p>
<p>Say we wanted to test the hypothesis that <span class="math inline">\(H_0:\beta_1 = 2.5\)</span> against the alternative that <span class="math inline">\(\beta_1\neq2.5\)</span>. Then the null, or reduced model, would be <span class="math inline">\(E\{Y_i\}=\beta_0+2.5X_i\)</span>. The alternative, or full model, would be <span class="math inline">\(E\{Y_i\}=\beta_0+\beta_1X_i\)</span>. Thus, the null (reduced) model contains only one “free” parameter because <span class="math inline">\(\beta_1\)</span> has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two “free” parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model.</p>
<p>Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing <span class="math inline">\(H_0: \beta_1=0\)</span> against <span class="math inline">\(H_a:\beta_1\neq0\)</span> we have the partition <span class="math display">\[
  \underbrace{Y_i-\bar{Y}}_{Total} = \underbrace{\hat{Y}_i - \bar{Y}}_{Regression} + \underbrace{Y_i-\hat{Y}_i}_{Error}
\]</span> The reason we use <span class="math inline">\(\bar{Y}\)</span> for the null model is that <span class="math inline">\(\bar{Y}\)</span> is the unbiased estimator of <span class="math inline">\(\beta_0\)</span> for the null model, <span class="math inline">\(E\{Y_i\} = \beta_0\)</span>. Thus we would compute the following sums of squares: <span class="math display">\[
  SSTO = \sum(Y_i-\bar{Y})^2
\]</span> <span class="math display">\[
  SSR = \sum(\hat{Y}_i-\bar{Y})^2
\]</span> <span class="math display">\[
  SSE = \sum(Y_i-\hat{Y}_i)^2
\]</span> and note that <span class="math inline">\(SSTO = SSR + SSE\)</span>. Important to note is that <span class="math inline">\(SSTO\)</span> uses the difference between the observations <span class="math inline">\(Y_i\)</span> and the null (reduced) model. The <span class="math inline">\(SSR\)</span> uses the diffences between the alternative (full) and null (reduced) model. The <span class="math inline">\(SSE\)</span> uses the differences between the observations <span class="math inline">\(Y_i\)</span> and the alternative (full) model. From these we could set up a General <span class="math inline">\(F\)</span> table of the form</p>
<table style="width:60%;">
<colgroup>
<col width="12%" />
<col width="12%" />
<col width="6%" />
<col width="13%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th> </th>
<th>Sum Sq</th>
<th>Df</th>
<th>Mean Sq</th>
<th>F Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Model Error</td>
<td><span class="math inline">\(SSR\)</span></td>
<td><span class="math inline">\(df_R-df_F\)</span></td>
<td><span class="math inline">\(\frac{SSR}{df_R-df_F}\)</span></td>
<td><span class="math inline">\(\frac{SSR}{df_R-df_F}\cdot\frac{df_F}{SSE}\)</span></td>
</tr>
<tr class="even">
<td>Residual Error</td>
<td><span class="math inline">\(SSE\)</span></td>
<td><span class="math inline">\(df_F\)</span></td>
<td><span class="math inline">\(\frac{SSE}{df_F}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Total Error</td>
<td><span class="math inline">\(SSTO\)</span></td>
<td><span class="math inline">\(df_R\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
<p><br /></p>
</div>
<div id="transformations-expand" class="section level4">
<h4>Transformations <a href="javascript:showhide('transformations')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span class="math inline">\(Y&#39;\)</span>, <span class="math inline">\(X&#39;\)</span>, and returning to the original space…</span></p>
<div id="transformations" style="display:none;">
<p>Y transformations are denoted by y-prime, written <span class="math inline">\(Y&#39;\)</span>, and consist of raising <span class="math inline">\(Y\)</span> to some power called <span class="math inline">\(\lambda\)</span>.</p>
<p><span class="math display">\[
  Y&#39; = Y^\lambda \quad \text{(Y Transformation)}
\]</span></p>
<table>
<thead>
<tr class="header">
<th align="center">Value of <span class="math inline">\(\lambda\)</span></th>
<th>Transformation to Use</th>
<th>R Code</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">-2</td>
<td><span class="math inline">\(Y&#39; = Y^{-2} = 1/Y^2\)</span></td>
<td><code>lm(Y^-2 ~ X)</code></td>
</tr>
<tr class="even">
<td align="center">-1</td>
<td><span class="math inline">\(Y&#39; = Y^{-1} = 1/Y\)</span></td>
<td><code>lm(Y^-1 ~ X)</code></td>
</tr>
<tr class="odd">
<td align="center">0</td>
<td><span class="math inline">\(Y&#39; = \log(Y)\)</span></td>
<td><code>lm(log(Y) ~ X)</code></td>
</tr>
<tr class="even">
<td align="center">0.5</td>
<td><span class="math inline">\(Y&#39; = \sqrt(Y)\)</span></td>
<td><code>lm(sqrt(Y) ~ X)</code></td>
</tr>
<tr class="odd">
<td align="center">1</td>
<td><span class="math inline">\(Y&#39; = Y\)</span></td>
<td><code>lm(Y ~ X)</code></td>
</tr>
<tr class="even">
<td align="center">2</td>
<td><span class="math inline">\(Y&#39; = Y^2\)</span></td>
<td><code>lm(Y^2 ~ X)</code></td>
</tr>
</tbody>
</table>
<p>Using “maximum-likelihood” estimation, the Box-Cox procedure can actually automatically detect the “optimal” value of <span class="math inline">\(\lambda\)</span> to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise.</p>
<div class="tab">
<button class="tablinks" onclick="openTab(event, 'ScatterplotView')">
Scatterplot Recognition
</button>
<button class="tablinks" onclick="openTab(event, 'BoxCoxView')">
Box-Cox Suggestion
</button>
<button class="tablinks" onclick="openTab(event, 'YTransExample')">
An Example
</button>
</div>
<div id="ScatterplotView" class="tabcontent" style="display:block;">
<p>
<h6 id="scatterplot-recognition">Scatterplot Recognition</h6>
<p>The following panel of scatterplots can give you a good feel for when to try different values of <span class="math inline">\(\lambda\)</span>.</p>
<pre class="r"><code>set.seed(15)
N &lt;- 300
X &lt;- runif(N, 5, 50)
Y &lt;- 25 + 3.5*X + rnorm(N, 0, 20)

Ya &lt;- 1/sqrt(Y)   #1/Y^2   Lam = -2
Yb &lt;- 1/Y         #1/Y     Lam = -1
Yc &lt;- exp(.02*Y)  #log(Y)  Lam =  0
Yd &lt;- Y^2         #sqrt(Y) Lam =  0.5
Ye &lt;- Y           #Y       Lam =  1
Yf &lt;- sqrt(Y)     #Y^2     Lam =  2


par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0))

plot(Ya ~ X, main=expression(paste(&quot;Use &quot;, lambda == -2)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Ya^-2 ~ X))
curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)


plot(Yb ~ X, main=expression(paste(&quot;Use &quot;, lambda == -1)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Yb^-1 ~ X))
curve(1/(b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Yc ~ X, main=expression(paste(&quot;Use &quot;, lambda == 0, &quot; i.e., log(...)&quot;)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(log(Yc) ~ X))
curve(exp(b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)


plot(Yd ~ X, main=expression(paste(&quot;Use &quot;, lambda == 0.5)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(sqrt(Yd) ~ X))
curve((b[1] + b[2]*x)^2, add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Ye ~ X, main=expression(paste(&quot;Use &quot;, lambda == 1, &quot; (No Transformation)&quot;)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Ye ~ X))
curve((b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Yf ~ X, main=expression(paste(&quot;Use &quot;, lambda == 2)), 
ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Yf^2 ~ X))
curve(sqrt(b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</p>
</div>
<div id="BoxCoxView" class="tabcontent">
<p>
<h6 id="box-cox-suggestion">Box-Cox Suggestion</h6>
<p>The <code>boxCox(...)</code> function in <code>library(car)</code> can also be helpful on finding values of <span class="math inline">\(\lambda\)</span> to try.</p>
<pre class="r"><code>par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0))

boxCox(lm(Ya ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == -2)), line=.5)

boxCox(lm(Yb ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == -1)), line=.5)

boxCox(lm(Yc ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == 0, &quot; i.e., log(...)&quot;)), line=.5)

boxCox(lm(Yd ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == 0.5)), line=.5)

boxCox(lm(Ye ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == 1, &quot; (No Transformation)&quot;)), line=.5)

boxCox(lm(Yf ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == 2)), line=.5)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</p>
</div>
<div id="YTransExample" class="tabcontent">
<p>
<h6 id="an-example">An Example</h6>
<p>Suppose <span class="math inline">\(\lambda = 0.5\)</span>, so that <span class="math inline">\(Y&#39; = \sqrt{Y}\)</span>.</p>
<p>A regression is performed using <code>sqrt(Y)</code>:</p>
<p><code>cars.lm.t &lt;- lm(sqrt(dist) ~ speed, data=cars)</code></p>
<p><code>summary(cars.lm.t)</code></p>
<table>
<thead>
<tr class="header">
<th> </th>
<th>Estimate</th>
<th>Std. Error</th>
<th>t value</th>
<th>Pr(&gt;</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>(Intercept)</strong></td>
<td>1.277</td>
<td>0.4844</td>
<td>2.636</td>
<td>0.01126</td>
</tr>
<tr class="even">
<td><strong>speed</strong></td>
<td>0.3224</td>
<td>0.02978</td>
<td>10.83</td>
<td>1.773e-14</td>
</tr>
</tbody>
</table>
<p>Then,</p>
<p><span class="math display">\[
  \hat{Y}_i&#39; = 1.277 + 0.3224 X_i
\]</span></p>
<p>And replacing <span class="math inline">\(\hat{Y}_i&#39; = \sqrt{\hat{Y}_i}\)</span> we have</p>
<p><span class="math display">\[
  \sqrt{\hat{Y}_i} = 1.277 + 0.3224 X_i
\]</span></p>
<p>Solving for <span class="math inline">\(\hat{Y}_i\)</span> gives</p>
<p><span class="math display">\[
  \hat{Y}_i = (1.277 + 0.3224 X_i)^2
\]</span></p>
<p>Which, using <code>curve((1.277 + 0.3224*x)^2, add=TRUE)</code> (see code for details) looks like this:</p>
<pre class="r"><code>plot(dist ~ speed, data=cars, pch=20, col=&quot;firebrick&quot;, cex=1.2, las=1,
     xlab=&quot;Speed of the Vehicle (mph) \n the Moment the Brakes were Applied&quot;, ylab=&quot;Distance (ft) it took the Vehicle to Stop&quot;,
     main=&quot;Don&#39;t Step in front of a Moving 1920&#39;s Vehicle...&quot;)
mtext(side=3, text=&quot;...they take a few feet to stop.&quot;, cex=0.7, line=.5)
legend(&quot;topleft&quot;, legend=&quot;Stopping Distance Experiment&quot;, bty=&quot;n&quot;)

curve( (1.277 + 0.3224*x)^2, add=TRUE, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</p>
</div>
<p><br /></p>
<h5 id="x-transformations">X-Transformations</h5>
<p>X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them.</p>
<p>The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated.</p>
<p>The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation.</p>
<pre class="r"><code>set.seed(15)
N &lt;- 300
X &lt;- runif(N, 5, 50)
Y &lt;- 25 + 3.5*X + rnorm(N, 0, 20)

Xa &lt;- 1/sqrt(X)   #1/X^2   Lam = -2
Xb &lt;- 1/X         #1/X     Lam = -1
Xc &lt;- exp(.02*X)  #log(X)  Lam =  0
Xd &lt;- X^2         #sqrt(X) Lam =  0.5
Xe &lt;- X           #X       Lam =  1
Xf &lt;- sqrt(X)     #X^2     Lam =  2


par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0))

plot(Y ~ Xa, main=expression(paste(&quot;Use &quot;, X*minute == X^-2)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ I(Xa^-2)))
curve(b[1] + b[2]*x^-2, add=TRUE, col=&quot;green&quot;, lwd=2)


plot(Y ~ Xb, main=expression(paste(&quot;Use &quot;, X*minute == X^-1)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ I(Xb^-1)))
curve(b[1] + b[2]*x^-1, add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Y ~ Xc, main=expression(paste(&quot;Use &quot;, X*minute == log(X))), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ log(Xc)))
curve(b[1] + b[2]*log(x), add=TRUE, col=&quot;green&quot;, lwd=2)


plot(Y ~ Xd, main=expression(paste(&quot;Use &quot;, X*minute == sqrt(X))), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ sqrt(Xd)))
curve(b[1] + b[2]*sqrt(x), add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Y ~ Xe, main=expression(paste(&quot;Use &quot;, X*minute == X, &quot; (No Transformation)&quot;)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ Xe))
curve((b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Y ~ Xf, main=expression(paste(&quot;Use &quot;, X*minute == X^2)), 
ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ I(Xf^2)))
curve(b[1] + b[2]*x^2, add=TRUE, col=&quot;green&quot;, lwd=2)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
<p><br /></p>
</div>
<div id="prediction-and-confidence-intervals-for-haty_h-expand" class="section level4">
<h4>Prediction and Confidence Intervals for <span class="math inline">\(\hat{Y}_h\)</span> <a href="javascript:showhide('predictionintervals')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">predict(…, interval=“prediction”)… </span></p>
<div id="predictionintervals" style="display:none;">
<p>It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line.</p>
<table style="width:69%;">
<colgroup>
<col width="15%" />
<col width="12%" />
<col width="22%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th>Interval</th>
<th>R Code</th>
<th>Math Equation</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Prediction</td>
<td><span style="font-size:.8em;"><code>predict(..., interval=&quot;prediction&quot;)</code></span></td>
<td><span class="math inline">\(\hat{Y}_i \pm t^* \cdot s_{\text{Pred}\ Y}\)</span></td>
<td>Guess value for one observation</td>
</tr>
<tr class="even">
<td>Confidence</td>
<td><span style="font-size:.8em;"><code>predict(..., interval=&quot;confidence&quot;)</code></span></td>
<td><span class="math inline">\(\hat{Y}_i \pm t^* \cdot s_{\hat{Y}}\)</span></td>
<td>Estimate location of mean y-value</td>
</tr>
</tbody>
</table>
<p><code>predict(mylm, data.frame(XvarName = number), interval=...)</code></p>
<p><br /> <br /></p>
<p>For example, consider this graph. Then <a href="javascript:showhide('predictionintervalsgraph')" style="color:skyblue;">click here</a> to read about the graph.</p>
<div id="predictionintervalsgraph" style="padding-left:30px;padding-right:30px;font-size:.9em;display:none;">
<p>Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet.</p>
<p>The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet!</p>
<p>So, to predict that a car going 15 mph will take 41.4 feet to stop, doesn’t tell the whole story. Far more revealing is the complete statement, “Cars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.” This is called the “prediction interval” and is shown in the graph in blue. It is obtained in R with the codes:</p>
<p><code>cars.lm &lt;- lm(dist ~ speed, data=cars)</code></p>
<p><code>predict(cars.lm, data.frame(speed=15), interval=&quot;prediction&quot;)</code></p>
<pre class="r"><code>cars.lm &lt;- lm(dist ~ speed, data=cars)
pander(predict(cars.lm, data.frame(speed=15), interval=&quot;prediction&quot;))</code></pre>
<table style="width:33%;">
<colgroup>
<col width="11%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">fit</th>
<th align="center">lwr</th>
<th align="center">upr</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">41.41</td>
<td align="center">10.17</td>
<td align="center">72.64</td>
</tr>
</tbody>
</table>
</div>
<pre class="r"><code>plot(dist ~ speed, data=cars, pch=20, col=&quot;firebrick&quot;, cex=1.2, las=1,
     xlab=&quot;Speed of the Vehicle (mph) \n the Moment the Brakes were Applied&quot;, ylab=&quot;Distance (ft) it took the Vehicle to Stop&quot;,
     main=&quot;Don&#39;t Step in front of a Moving 1920&#39;s Vehicle...&quot;)
mtext(side=3, text=&quot;...they take a few feet to stop.&quot;, cex=0.7, line=.5)
legend(&quot;topleft&quot;, legend=&quot;Stopping Distance Experiment&quot;, bty=&quot;n&quot;)
points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=&quot;firebrick2&quot;, cex=1.5)

cars.lm &lt;- lm(dist ~ speed, data=cars)
abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3))
abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2))
abline(v=15, lty=2, col=&quot;firebrick&quot;)

preds &lt;- predict(cars.lm, data.frame(speed=15), interval=&quot;prediction&quot;)
lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12)
lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8))
lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8))</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<p><br /></p>
<hr />
</div>
</div>
</div>
</div>
<div id="section" class="section level2">
<h2></h2>
<div style="padding-left:125px;">
<p><strong>Examples:</strong> <a href="./Analyses/Linear%20Regression/Examples/BodyWeightSLR.html">bodyweight</a>, <a href="./Analyses/Linear%20Regression/Examples/carsSLR.html">cars</a></p>
</div>
<hr />
</div>
<div id="multiple-linear-regression" class="section level2 tabset tabset-fade tabset-pills">
<h2>Multiple Linear Regression</h2>
<div style="float:left;width:125px;" align="center">
<p><img src="Images/QuantYMultX.png" width=108px;></p>
</div>
<p>Multiple regression allows for more than one explanatory variable to be included in the modeling of the expected value of the quantitative response variable <span class="math inline">\(Y_i\)</span>.</p>
<div id="overview-1" class="section level3">
<h3>Overview</h3>
<div style="padding-left:125px;">
<p>A typical multiple regression model is given by the equation <span class="math display">\[
  Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_p X_{pi} + \epsilon_i
\]</span> where <span class="math inline">\(\epsilon_i\sim N(0,\sigma^2)\)</span>.</p>
<p>The coefficient <span class="math inline">\(\beta_j\)</span> is interpreted as the change in the expected value of <span class="math inline">\(Y\)</span> for a unit increase in <span class="math inline">\(X_{j}\)</span>, holding all other variables constant, for <span class="math inline">\(j=1,\ldots,p\)</span>.</p>
<p>See the <strong>Explanation</strong> tab for details about possible hypotheses here.</p>
<hr />
</div>
</div>
<div id="r-instructions-1" class="section level3">
<h3>R Instructions</h3>
<div style="padding-left:125px;">
<p><strong>Console</strong> Help Command: <code>?lm()</code></p>
<p>Everything is the same as in simple linear regression except that more variables are allowed in the call to <code>lm()</code>.</p>
<p><code>mylm &lt;- lm(Y ~ X1 + X2 + X1:X2 + ..., data=YourDataSet)</code></p>
<p><code>summary(mylm)</code></p>
<ul>
<li><code>mylm</code> is some name you come up with to store the results of the <code>lm()</code> test. Note that <code>lm()</code> stands for “linear model.”</li>
<li><code>Y</code> must be a “numeric” vector of the quantitative response variable.</li>
<li><code>X1</code> and <code>X2</code> are the explanatory variables. These can either be quantitative or qualitative. Note that R treats “numeric” variables as quantitative and “character” or “factor” variables as qualitative. R will automatcially recode qualitative variables to become “numeric” variables using a 0,1 encoding. See the Explanation tab for details.</li>
<li><code>YourDataSet</code> is the name of your data set.</li>
<li><code>X1:X2</code> is called the interaction term. See the Explanation tab for details.</li>
<li><code>...</code> emphasizes that as many explanatory variables as are desired can be included in the model.</li>
</ul>
<hr />
</div>
</div>
<div id="explanation-1" class="section level3">
<h3>Explanation</h3>
<div style="padding-left:125px;">
<p>The extension of linear regression to multiple regression is fairly direct, include more than one explanatory variable, yet very powerful.</p>
<h4 id="extending-the-mathematical-model-expand">Extending The Mathematical Model <a href="javascript:showhide('mathmodel2')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">Options for <span class="math inline">\(E\{Y_i\}\)</span> beyond <span class="math inline">\(\beta_0 + \beta_1 X_i\)</span>…</span></p>
<div id="mathmodel2" style="display:none;">
<p>The multiple linear regression model is given by “a model” for the mean y-value, denoted by <span class="math inline">\(E\{Y_i\}\)</span>, and a normally distributed error term, <span class="math inline">\(\epsilon_i\)</span>.</p>
<p><span class="math display">\[
  Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_p X_{pi}}_{E\{Y_i\}}}^\text{&quot;The Model&quot;} + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \sigma^2)
\]</span></p>
<p>The model can be many things. Here are some examples.</p>
<p><span class="math display">\[
 Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_i}_{E\{Y_i\}}}^\text{Simple Linear Model} + \epsilon_i
\]</span></p>
<p><span class="math display">\[
 Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_i + \beta_2 X_i^2}_{E\{Y_i\}}}^\text{Quadratic Model} + \epsilon_i
\]</span></p>
<p><span class="math display">\[
 Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{1i} X_{2i}}_{E\{Y_i\}}}^\text{Two-lines Model} + \epsilon_i \quad \text{where} \ X_{2i} = \left\{\begin{array}{ll} 1, &amp; \text{Group B} \\ 0, &amp; \text{Group A} \end{array}\right.
\]</span></p>
<pre class="r"><code>par(mfrow=c(1,3), mai=c(0.1,0.1,0.3,0.1), cex.main=1.5)

plot(Wind ~ Temp, data=airquality, pch=21, bg=&quot;gray83&quot;, col=&quot;skyblue&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, main=&quot;Simple Linear Model&quot;, cex.main=1)
lm.slr &lt;- lm(Wind ~ Temp, data=airquality)
abline(lm.slr, col=&quot;orange&quot;, lwd=2)


plot(Temp ~ Month, data=airquality, pch=21, bg=&quot;gray83&quot;, col=&quot;skyblue&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, main=&quot;Quadratic Model&quot;, cex.main=1)
lm.quad &lt;- lm(Temp ~ Month + I(Month^2), data=airquality)
b &lt;- coef(lm.quad)
curve(b[1] + b[2]*x + b[3]*x^2, col=&quot;orange&quot;, lwd=2, add=TRUE)


plot(mpg ~ qsec, data=mtcars, col=c(&quot;skyblue&quot;,&quot;orange&quot;)[as.factor(am)], pch=21, bg=&quot;gray83&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;Two-lines Model&quot;, cex.main=1)
lm.2lines &lt;- lm(mpg ~ qsec + am + qsec:am, data=mtcars)
b &lt;- coef(lm.2lines)
curve(b[1] + b[2]*x, col=&quot;skyblue&quot;, lwd=2, add=TRUE)
curve(b[1] + b[3] + (b[2] + b[4])*x, col=&quot;orange&quot;, lwd=2, add=TRUE)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>In fact, the model for <span class="math inline">\(E\{Y_i\}\)</span> can be any “linear combination” of <span class="math inline">\(\beta\)</span>’s, which means <span class="math inline">\(\beta\)</span> times some X-variable plus <span class="math inline">\(\beta\)</span> times some X-variable plus… plus the error term <span class="math inline">\(\epsilon_i\)</span>. So you could have <span class="math inline">\(\beta_1 \sin(X_i)\)</span>, <span class="math inline">\(\beta_2 e^{X_2i}\)</span>, and anything else you can dream up. But <span class="math inline">\(e^{\beta_1 X_i}\)</span> is not allowed. Of course, you could fit the model <span class="math inline">\(Y_i = e^{\beta_0 + \beta_1 X_i + \epsilon_i}\)</span> using a <span class="math inline">\(\log(Y_i)\)</span> transformation, but then, the regression would actually be <span class="math inline">\(\log(Y_i) = \beta_0 + \beta_1 X_i + \epsilon_i\)</span> which is again a “linear regression”.</p>
<div style="padding-left:15px; color:#a8a8a8;">
<p><strong>Note</strong>: Interactions, transformations of other variables, and qualitative variables can all be included in the model. For example, if a model included three explanatory variables, <span class="math inline">\(X_1,X_2,X_3\)</span>, then <span class="math inline">\(X_{3i}\)</span> could be defined to be the interaction between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span>, i.e., <span class="math inline">\(X_{3i} = X_{1i}\cdot X_{2i}\)</span>. If <span class="math inline">\(X_3\)</span> was instead to represent a qualitative variable with two levels, then we could use a 0 to represent one level of the variable and a 1 to represent the other level. If we had good reason to do so, we could even let <span class="math inline">\(X_{3i} = X_{1i}^2\)</span> or <span class="math inline">\(\log(X_{1i})\)</span> or some other transformation of another <span class="math inline">\(X\)</span> variable.</p>
</div>
<p>Say we are interested in the price of a vehicle, particularly the price of a Cadillac. Simple linear regression would just use the mileage of the vehicle to predict the price. This will probably not be very successful as different makes of Cadillacs vary widely in their prices. However, if we include other explanatory variables in our model, like model of the vehicle, we should be able to do very well at predicting the price of a particular vehicle. (Certainly other variables like the number of doors, engine size, automatic or manual transmission and so on could also be valuable explanatory variables.)</p>
</div>
<p><br /></p>
<h4 id="interpretation">Interpretation</h4>
<p>The only change to interpretation from the simple linear regression model is that each coefficient, <span class="math inline">\(\beta_j\)</span> <span class="math inline">\(j=1,\ldots,p\)</span>, represents the change in the <span class="math inline">\(E\{Y\}\)</span> for a unit change in <span class="math inline">\(X_j\)</span>, <em>holding all other variables constant.</em></p>
<p><br /></p>
<h4 id="assumptions">Assumptions</h4>
<p>The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption.</p>
<ol style="list-style-type: decimal">
<li>The regression relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is linear.</li>
<li>The error terms are normally distributed with <span class="math inline">\(E\{\epsilon_i\}=0\)</span>.</li>
<li>The variance of the error terms is constant over all <span class="math inline">\(X\)</span> values.</li>
<li>The <span class="math inline">\(X\)</span> values can be considered fixed and measured without error.</li>
<li>The error terms are independent.</li>
<li>All important variables are included in the model.</li>
</ol>
<p><br /></p>
<h4 id="check">Checking the Assumptions</h4>
<p>The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model.</p>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-25-1.png" width="144" />
</td>
<td width="75%">
<p>Let <span class="math inline">\(X_{new}\)</span> be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against <span class="math inline">\(X_{new}\)</span> allows us to determine if <span class="math inline">\(X_{new}\)</span> has any information to add to the current model. If there is a trend in the plot, then <span class="math inline">\(X_{new}\)</span> should be added to the model. If there is no trend in the plot, then the <span class="math inline">\(X_{new}\)</span> should be left out.</p>
<p>| <a href="javascript:showhide('addedvariableplots')" style="font-size:.8em;color:steelblue2;">Show Examples</a> |</p>
</td>
</tr>
</table>
<div id="addedvariableplots" style="display:none;">
<p><a href="javascript:showhide('addedvariableplotsread')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="addedvariableplotsread" style="display:none;">
<p>An added variable plot checks to see if a new variable has any information to add to the current multiple regression model.</p>
<p>The plot is made by taking the residuals from the current multiple regression model (<span class="math inline">\(y\)</span>-axis) and plotting them against the new explanatory variable (<span class="math inline">\(x\)</span>-axis).</p>
<ul>
<li><p>If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model.</p></li>
<li><p>If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model.</p></li>
</ul>
<p>The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model.</p>
</div>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-26-1.png" width="672" /><img src="LinearRegression_files/figure-html/unnamed-chunk-26-2.png" width="672" /></p>
</div>
<p><br /></p>
<h4 id="infModelParam">Inference for the Model Parameters</h4>
<p>Inference in the multiple regression model can be for any of the model coefficients, <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\beta_p\)</span> or for several coefficients simultaneously.</p>
<p><br /></p>
<h5 id="t-tests">t Tests</h5>
<p>The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as <span class="math display">\[
  H_0: \beta_j = 0
\]</span> <span class="math display">\[
  H_a: \beta_j \neq 0
\]</span> Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance.</p>
<p><br /></p>
<h5 id="f-tests">F Tests</h5>
<p>Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously.</p>
<p>The most commonly used F Test is the one given by the hypotheses <span class="math display">\[
  H_0: \beta_0 = \beta_1 = \cdots = \beta_p = 0
\]</span> <span class="math display">\[
  H_a: \beta_j \neq 0 \ \text{for at least one}\ j \in \{0,1,\ldots,p\}
\]</span> However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class.</p>
<p><br /></p>
<h4 id="rsquared">Assessing the Model Fit</h4>
<p>There are many measures of the quality of a regression model. One of the most popular measurements is the <span class="math inline">\(R^2\)</span> value (“R-squared”). The <span class="math inline">\(R^2\)</span> value is a measure of the proportion of variation of the <span class="math inline">\(Y\)</span>-variable that is explained by the model. Specifically, <span class="math display">\[
  R^2 = \frac{\text{SSR}}{\text{SSTO}} = 1-\frac{\text{SSE}}{\text{SSTO}}
\]</span> The range of <span class="math inline">\(R^2\)</span> is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model.</p>
<p>One difficulty of <span class="math inline">\(R^2\)</span> in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the <span class="math inline">\(R^2\)</span> value to protect against this difficulty. The value of the adjusted <span class="math inline">\(R^2\)</span> is given by <span class="math display">\[
  R^2_{adj} = 1 - \frac{(n-1)}{(n-p)}\frac{\text{SSE}}{\text{SSTO}}
\]</span> The interpretation of <span class="math inline">\(R^2_{adj}\)</span> is essentially the same as the interpretation of <span class="math inline">\(R^2\)</span>, with the understanding that a correction has been made for the number of parameters included in the model, <span class="math inline">\((n-p)\)</span>.</p>
<p><br /> <br /></p>
<hr />
</div>
</div>
</div>
<div id="section-1" class="section level2">
<h2></h2>
<div style="padding-left:125px;">
<p><strong>Examples:</strong> <a href="./Analyses/Linear%20Regression/Examples/CivicVsCorollaMLR.html">Civic Vs Corolla</a> <a href="./Analyses/Linear%20Regression/Examples/cadillacsMLR.html">cadillacs</a></p>
</div>
<hr />
<footer>
</footer>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
